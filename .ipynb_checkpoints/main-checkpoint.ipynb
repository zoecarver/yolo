{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T03:31:38.817573Z",
     "start_time": "2018-12-22T03:31:38.812872Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, LeakyReLU, UpSampling2D, InputLayer, Concatenate, Input, merge, concatenate, Lambda, Reshape, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the config\n",
    "\n",
    "The network config is in `yolov3.cfg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T03:45:03.230583Z",
     "start_time": "2018-12-22T03:45:03.222071Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_config(cfg_path, verbose=False):\n",
    "    file = open(cfg_path, 'r')\n",
    "    lines = file.readlines()\n",
    "        \n",
    "    # get rid of comments and blank lines and white space\n",
    "    lines = [x for x in lines if len(x) > 1]\n",
    "    lines = [x for x in lines if x[0] != '#']\n",
    "    lines = [x.rstrip().lstrip() for x in lines]\n",
    "\n",
    "    block = {}\n",
    "    blocks = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if verbose: print('valuating line: %s' % line)\n",
    "        \n",
    "        if line[0] == '[': # new block start\n",
    "            if len(block) != 0: # if the block inst empty (has data) then reset it\n",
    "                blocks.append(block)\n",
    "                block = {}\n",
    "                \n",
    "            block['type'] = line[1:-1]\n",
    "        else:\n",
    "            key, value = line.split('=')\n",
    "            key, value = key.rstrip(), value.lstrip()\n",
    "            block[key] = value\n",
    "\n",
    "    blocks += [block]\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T03:45:32.733760Z",
     "start_time": "2018-12-22T03:45:32.720405Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = parse_config('custom_yolo.cfg')\n",
    "\n",
    "# configs from https://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb\n",
    "LABELS = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "IMAGE_H, IMAGE_W = 416, 416\n",
    "GRID_H,  GRID_W  = 13 , 13\n",
    "BOX              = 5\n",
    "CLASS            = len(LABELS)\n",
    "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
    "OBJ_THRESHOLD    = 0.3#0.5\n",
    "NMS_THRESHOLD    = 0.3#0.45\n",
    "ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "\n",
    "NO_OBJECT_SCALE  = 1.0\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "CLASS_SCALE      = 1.0\n",
    "\n",
    "BATCH_SIZE       = 16\n",
    "WARM_UP_BATCHES  = 0\n",
    "TRUE_BOX_BUFFER  = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix shortcut issue\n",
    "this is a quick fix for the shortcut issue I was running into but I want to figure out why and how this works so I can create a better implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T03:45:33.971827Z",
     "start_time": "2018-12-22T03:45:33.968248Z"
    }
   },
   "outputs": [],
   "source": [
    "def space_to_depth_x2(x):\n",
    "    return tf.space_to_depth(x, block_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "(add specific info about the model)\n",
    "\n",
    "### Questions:\n",
    "1. Why is padding important in a convolutional layer\n",
    "2. What is `BatchNormalization` and why is it important\n",
    "3. What does `bilinear` mean?\n",
    "4. What is `B X C X H X W` and how is it difforent than other formats\n",
    "5. What is the difforence between route and shortcut and how are they implemented?\n",
    "    * see the degradation problem, and an [explaination here](https://www.quora.com/What-are-shortcut-connections-how-do-they-work-and-what-is-their-role-in-the-paper-Deep-Residual-Learning-for-Image-Recognition).\n",
    "6. What does `space_to_depth_x2` do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T04:03:59.186752Z",
     "start_time": "2018-12-22T04:03:52.723496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/31) processing: convolutional\n",
      "(1/31) processing: maxpooling\n",
      "(2/31) processing: convolutional\n",
      "(3/31) processing: maxpooling\n",
      "(4/31) processing: convolutional\n",
      "(5/31) processing: convolutional\n",
      "(6/31) processing: convolutional\n",
      "(7/31) processing: maxpooling\n",
      "(8/31) processing: convolutional\n",
      "(9/31) processing: convolutional\n",
      "(10/31) processing: convolutional\n",
      "(11/31) processing: maxpooling\n",
      "(12/31) processing: convolutional\n",
      "(13/31) processing: convolutional\n",
      "(14/31) processing: convolutional\n",
      "(15/31) processing: convolutional\n",
      "(16/31) processing: convolutional\n",
      "(17/31) processing: skip_connection\n",
      "(18/31) processing: maxpooling\n",
      "(19/31) processing: convolutional\n",
      "(20/31) processing: convolutional\n",
      "(21/31) processing: convolutional\n",
      "(22/31) processing: convolutional\n",
      "(23/31) processing: convolutional\n",
      "(24/31) processing: convolutional\n",
      "(25/31) processing: convolutional\n",
      "(26/31) processing: skip_connection\n",
      "(27/31) processing: convolutional\n",
      "(28/31) processing: space_to_depth\n",
      "(29/31) processing: concatenate\n",
      "(30/31) processing: convolutional\n",
      "(31/31) processing: net\n"
     ]
    }
   ],
   "source": [
    "input_layer = blocks[0]\n",
    "input_shape = (int(input_layer['shape']), \n",
    "    int(input_layer['shape']), \n",
    "    int(input_layer['channels']))\n",
    "\n",
    "model_input = Input(input_shape)\n",
    "x = model_input\n",
    "\n",
    "skip_connection = None\n",
    "skip_connection_x = None\n",
    "output = None\n",
    "model = None\n",
    "\n",
    "conv_count = 0\n",
    "\n",
    "for index, block in enumerate(blocks[1:]):\n",
    "    print('(%i/%i) processing: %s' % (index, len(blocks) - 2, block['type']))\n",
    "    \n",
    "    if block['type'] == 'convolutional':        \n",
    "        filters = int(block['filters'])\n",
    "        kernel = int(block['kernel'])\n",
    "        strides = int(block['strides'])\n",
    "        \n",
    "        x = Conv2D(filters, \n",
    "                   (kernel, kernel), \n",
    "                   strides=(strides, strides), \n",
    "                   padding='same', \n",
    "                   name='conv_%i' % conv_count,\n",
    "                   use_bias=False)(x)\n",
    "        x = BatchNormalization(name='batch_norm_%i' % conv_count)(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "        \n",
    "        conv_count += 1\n",
    "        \n",
    "    if block['type'] == 'maxpooling':\n",
    "        pool = int(block['pool'])\n",
    "        \n",
    "        x = MaxPooling2D(pool_size=(pool, pool))(x)\n",
    "        \n",
    "    if block['type'] == 'skip_connection':\n",
    "        open_connection = int(block['open'])\n",
    "        \n",
    "        if open_connection:\n",
    "            skip_connection = x # hold current x for later\n",
    "        else:\n",
    "            skip_connection_x = x # make sure we know what this is for concat\n",
    "            x = skip_connection # set x to skip connect from earlier\n",
    "    \n",
    "    if block['type'] == 'space_to_depth':\n",
    "        x = Lambda(space_to_depth_x2)(x)\n",
    "        \n",
    "    if block['type'] == 'concatenate':\n",
    "        x = concatenate([x, skip_connection_x])\n",
    "        \n",
    "    if block['type'] == 'net':\n",
    "        x = Conv2D(\n",
    "            BOX * (4 + 1 + CLASS), (1, 1), strides=(1, 1), padding='same')(x)\n",
    "        output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
    "#         output = Lambda(lambda a: a[0])([output])\n",
    "        \n",
    "        model = Model(model_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T04:04:06.423329Z",
     "start_time": "2018-12-22T04:04:03.827494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_32 (InputLayer)           (None, 416, 416, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_0 (Conv2D)                 (None, 416, 416, 32) 864         input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_0 (BatchNormalizatio (None, 416, 416, 32) 128         conv_0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_821 (LeakyReLU)     (None, 416, 416, 32) 0           batch_norm_0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_79 (MaxPooling2D) (None, 208, 208, 32) 0           leaky_re_lu_821[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 208, 208, 64) 18432       max_pooling2d_79[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_1 (BatchNormalizatio (None, 208, 208, 64) 256         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_822 (LeakyReLU)     (None, 208, 208, 64) 0           batch_norm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_80 (MaxPooling2D) (None, 104, 104, 64) 0           leaky_re_lu_822[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 104, 104, 128 73728       max_pooling2d_80[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_2 (BatchNormalizatio (None, 104, 104, 128 512         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_823 (LeakyReLU)     (None, 104, 104, 128 0           batch_norm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 104, 104, 64) 8192        leaky_re_lu_823[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_3 (BatchNormalizatio (None, 104, 104, 64) 256         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_824 (LeakyReLU)     (None, 104, 104, 64) 0           batch_norm_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 104, 104, 128 73728       leaky_re_lu_824[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_4 (BatchNormalizatio (None, 104, 104, 128 512         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_825 (LeakyReLU)     (None, 104, 104, 128 0           batch_norm_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_81 (MaxPooling2D) (None, 52, 52, 128)  0           leaky_re_lu_825[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, 52, 52, 256)  294912      max_pooling2d_81[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_5 (BatchNormalizatio (None, 52, 52, 256)  1024        conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_826 (LeakyReLU)     (None, 52, 52, 256)  0           batch_norm_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, 52, 52, 128)  32768       leaky_re_lu_826[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_6 (BatchNormalizatio (None, 52, 52, 128)  512         conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_827 (LeakyReLU)     (None, 52, 52, 128)  0           batch_norm_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, 52, 52, 256)  294912      leaky_re_lu_827[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_7 (BatchNormalizatio (None, 52, 52, 256)  1024        conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_828 (LeakyReLU)     (None, 52, 52, 256)  0           batch_norm_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_82 (MaxPooling2D) (None, 26, 26, 256)  0           leaky_re_lu_828[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, 26, 26, 512)  1179648     max_pooling2d_82[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_8 (BatchNormalizatio (None, 26, 26, 512)  2048        conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_829 (LeakyReLU)     (None, 26, 26, 512)  0           batch_norm_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, 26, 26, 256)  131072      leaky_re_lu_829[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_9 (BatchNormalizatio (None, 26, 26, 256)  1024        conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_830 (LeakyReLU)     (None, 26, 26, 256)  0           batch_norm_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_830[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_10 (BatchNormalizati (None, 26, 26, 512)  2048        conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_831 (LeakyReLU)     (None, 26, 26, 512)  0           batch_norm_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_11 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_831[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_11 (BatchNormalizati (None, 26, 26, 256)  1024        conv_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_832 (LeakyReLU)     (None, 26, 26, 256)  0           batch_norm_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_12 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_832[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_12 (BatchNormalizati (None, 26, 26, 512)  2048        conv_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_833 (LeakyReLU)     (None, 26, 26, 512)  0           batch_norm_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_83 (MaxPooling2D) (None, 13, 13, 512)  0           leaky_re_lu_833[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_13 (Conv2D)                (None, 13, 13, 1024) 4718592     max_pooling2d_83[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_13 (BatchNormalizati (None, 13, 13, 1024) 4096        conv_13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_834 (LeakyReLU)     (None, 13, 13, 1024) 0           batch_norm_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_14 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_834[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_14 (BatchNormalizati (None, 13, 13, 512)  2048        conv_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_835 (LeakyReLU)     (None, 13, 13, 512)  0           batch_norm_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_15 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_835[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_15 (BatchNormalizati (None, 13, 13, 1024) 4096        conv_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_836 (LeakyReLU)     (None, 13, 13, 1024) 0           batch_norm_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_836[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_16 (BatchNormalizati (None, 13, 13, 512)  2048        conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_837 (LeakyReLU)     (None, 13, 13, 512)  0           batch_norm_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_837[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_17 (BatchNormalizati (None, 13, 13, 1024) 4096        conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_838 (LeakyReLU)     (None, 13, 13, 1024) 0           batch_norm_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_18 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_838[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_18 (BatchNormalizati (None, 13, 13, 1024) 4096        conv_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 26, 26, 64)   32768       leaky_re_lu_833[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_839 (LeakyReLU)     (None, 13, 13, 1024) 0           batch_norm_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_20 (BatchNormalizati (None, 26, 26, 64)   256         conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_839[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_841 (LeakyReLU)     (None, 26, 26, 64)   0           batch_norm_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_19 (BatchNormalizati (None, 13, 13, 1024) 4096        conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 13, 13, 256)  0           leaky_re_lu_841[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_840 (LeakyReLU)     (None, 13, 13, 1024) 0           batch_norm_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 13, 13, 1280) 0           lambda_11[0][0]                  \n",
      "                                                                 leaky_re_lu_840[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_21 (Conv2D)                (None, 13, 13, 1024) 11796480    concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_norm_21 (BatchNormalizati (None, 13, 13, 1024) 4096        conv_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_842 (LeakyReLU)     (None, 13, 13, 1024) 0           batch_norm_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_736 (Conv2D)             (None, 13, 13, 425)  435625      leaky_re_lu_842[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 13, 13, 5, 85 0           conv2d_736[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 50,983,561\n",
      "Trainable params: 50,962,889\n",
      "Non-trainable params: 20,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Output and generating predictions\n",
    "\n",
    "### Output Tensor\n",
    "![image](https://cdn-images-1.medium.com/max/1600/1*cGfWw6lGmV1xUKRsd--JxQ.png)\n",
    "\n",
    "Each coordinate is represented like this:\n",
    "![image.png](https://blog.paperspace.com/content/images/2018/04/bbox_-2.png)\n",
    "the coordinates are the x, y in the matrix of anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T06:21:27.747392Z",
     "start_time": "2018-12-21T06:21:27.653171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 416, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"dog-cycle-car.png\")\n",
    "img = cv2.resize(img, (416, 416)) # resize to the input dimension\n",
    "img_ =  img[:, :, ::-1].transpose((2, 0, 1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T06:21:30.988553Z",
     "start_time": "2018-12-21T06:21:28.086633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14, 14, 255)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction = main_modle.predict(np.array([img]))\n",
    "test_prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T18:53:18.733929Z",
     "start_time": "2018-12-20T18:53:18.729557Z"
    }
   },
   "source": [
    "The above output will need to be changed into a an array like the one in the image above\n",
    "\n",
    "### Questions\n",
    "1. what is the 3rd attribute\n",
    "2. could you use another network to decrease grid size?\n",
    "3. what is this doing? `tf.math.exp(prediction[:, :, 2:4]) * anchors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T06:26:40.939334Z",
     "start_time": "2018-12-21T06:26:40.920246Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_prediction(prediction, network_info):\n",
    "    \"\"\"\n",
    "    :param network_info: first block\n",
    "    \"\"\"\n",
    "#     from IPython.core.debugger import Tracer; Tracer()()\n",
    "    \n",
    "    anchors = [(10,13),  (16,30),  (33,23),  (30,61),  (62,45),  (59,119),  (116,90),  (156,198),  (373,326)]\n",
    "    \n",
    "    network_height = int(network_info['height'])\n",
    "    network_class_count = int(network_info['classes'])\n",
    "    \n",
    "    shape = prediction.shape\n",
    "    batch_size = shape[0]\n",
    "    stride = network_height // shape[2] # strides to map output position to real position\n",
    "    grid_size = network_height // stride\n",
    "    box_attributes = 5 + network_class_count # each box starts out with 5 attributes (xmin, xmax, ymin, ymax, ?)\n",
    "    anchor_count = len(anchors) # TODO: get anchors from yolo layer\n",
    "    \n",
    "    prediction = prediction.reshape((\n",
    "        batch_size, box_attributes * anchor_count, grid_size * grid_size))\n",
    "    prediction = tf.transpose(tf.reshape(prediction, [1, 2]))\n",
    "    prediction = prediction.reshape((\n",
    "        batch_size, grid_size * grid_size * num_anchors, box_attributes))\n",
    "    \n",
    "    anchors = [(a[0] / stride, a[1] / stride) for a in anchors] # update anchors for strides\n",
    "    prediction[:, :, 0] = tf.math.sigmoid(prediction[:, :, 0]) # apply sigmoid to x, y and confidence\n",
    "    prediction[:, :, 1] = tf.math.sigmoid(prediction[:, :, 1])\n",
    "    prediction[:, :, 4] = tf.math.sigmoid(prediction[:, :, 4])\n",
    "    \n",
    "    grid = np.arange(grid_size)\n",
    "    a, b = np.meshgrid(grid, grid)\n",
    "    \n",
    "    x_offset = a.reshape((-1, 1))\n",
    "    y_offset = b.reshape((-1, 1))\n",
    "    \n",
    "    x_y_offset = np.concatenate((x_offset, y_offset), axis=1).repeat(1, num_anchors).reshape((1, -1, 2))\n",
    "    \n",
    "    prediction[:, :, :2] += x_y_offset\n",
    "\n",
    "    anchors = anchors.repeat(grid_size * grid_size, 1)\n",
    "    anchors = anchors.reshape((1,) + anchors.shape)\n",
    "    \n",
    "    prediction[:, :, 2:4] = tf.math.exp(prediction[:, :, 2:4]) * anchors\n",
    "    # apply sigmoid activation to all the classes\n",
    "    prediction[:, :, 5:5 + num_classes] = tf.math.sigmoid((prediction[:, :, 5:5 + num_classes]))\n",
    "\n",
    "    prediction[:, :, :4] *= stride # to make it reasonable for boxes\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T06:26:42.897626Z",
     "start_time": "2018-12-21T06:26:41.924108Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 49980 into shape (1,765,196)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-0b02cfac7e8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_modle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-7afce38653a2>\u001b[0m in \u001b[0;36mformat_prediction\u001b[0;34m(prediction, network_info)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     prediction = prediction.reshape((\n\u001b[0;32m---> 20\u001b[0;31m         batch_size, box_attributes * anchor_count, grid_size * grid_size))\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     prediction = prediction.reshape((\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 49980 into shape (1,765,196)"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(\"dog-cycle-car.png\")\n",
    "img = cv2.resize(img, (416, 416)) # resize to the input dimension\n",
    "img_ =  img[:, :, ::-1].transpose((2, 0, 1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "\n",
    "blocks[0]['classes'] = 80 # TODO load from yolo layer\n",
    "\n",
    "test_prediction = main_modle.predict(np.array([img]))\n",
    "test_prediction = format_prediction(test_prediction, blocks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Weights\n",
    "\n",
    "We want to create a class that will allow us to load the weights and read chucks of values.\n",
    "\n",
    "## Questions\n",
    "1. How are weights stored?\n",
    "2. beta, gamma, mean, var?\n",
    "3. why is there no bias needed if there are already weights\n",
    "    * I think that maybe what the below if statement does is not check for weights but check for `use_bias` which would explain both things\n",
    "4. does this to check for previos weights or something else? `len(conv_layer.get_weights()) > 1`\n",
    "5. why are the last layer's weights randomized? \n",
    "    * I _think_ this is so that some small training will be able to get a good result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T04:13:35.286502Z",
     "start_time": "2018-12-22T04:13:35.281912Z"
    }
   },
   "outputs": [],
   "source": [
    "class WeightReader:\n",
    "    def __init__(self, file):\n",
    "        self.offset = 4 # starting offset\n",
    "        self.weights = np.fromfile(file, dtype='float32') # load a file using numpy\n",
    "        \n",
    "    def read(self, size):\n",
    "        self.offset += size # increase offset\n",
    "        return self.weights[self.offset - size: self.offset] # read from values\n",
    "    \n",
    "    def reset(self):\n",
    "        self.offset = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T04:15:40.265743Z",
     "start_time": "2018-12-22T04:13:55.556726Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Much of the below code block was not written by me. See link to orininal below:\n",
    "https://github.com/experiencor/keras-yolo2/blob/master/Yolo%20Step-by-Step.ipynb\n",
    "\"\"\"\n",
    "\n",
    "weight_reader = WeightReader('yolo.weights')\n",
    "\n",
    "# thanks to @experiencor for help with this\n",
    "# this will take a while (~2 mins)\n",
    "for index in range(conv_count):\n",
    "    conv_layer = model.get_layer('conv_%i' % index)\n",
    "    norm_layer = model.get_layer('batch_norm_%i' % index)\n",
    "    \n",
    "    size = np.prod(norm_layer.get_weights()[0].shape) # get product of shape (total values)\n",
    "    \n",
    "    # read sizes\n",
    "    beta  = weight_reader.read(size)\n",
    "    gamma = weight_reader.read(size)\n",
    "    mean  = weight_reader.read(size)\n",
    "    var   = weight_reader.read(size)\n",
    "    \n",
    "    norm_layer.set_weights([gamma, beta, mean, var])\n",
    "    \n",
    "    if len(conv_layer.get_weights()) > 1: # ~~we already have some weights here~~ I think this actually checks for the `use_bias` flag\n",
    "        bias   = weight_reader.read(np.prod(conv_layer.get_weights()[1].shape)) # get enough weights to fill the current weight shape\n",
    "        kernel = weight_reader.read(np.prod(conv_layer.get_weights()[0].shape))\n",
    "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape))) # reshape\n",
    "        kernel = kernel.transpose([2, 3, 1, 0])\n",
    "        conv_layer.set_weights([kernel, bias])\n",
    "    else:\n",
    "        kernel = weight_reader.read(np.prod(conv_layer.get_weights()[0].shape)) # same as above without bias\n",
    "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
    "        kernel = kernel.transpose([2, 3, 1, 0])\n",
    "        conv_layer.set_weights([kernel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T04:18:44.349172Z",
     "start_time": "2018-12-22T04:18:43.766571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-a976934ff88e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrandom_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGRID_H\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mGRID_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrandom_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGRID_H\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mGRID_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_bias\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# TODO: try running this and try without running this\n",
    "\n",
    "layer = model.get_layer('conv_%i' % (conv_count - 1))\n",
    "weights = layer.get_weights()\n",
    "print(len(weights))\n",
    "\n",
    "random_kernel = np.random.normal(size=weights[0].shape) / (GRID_H * GRID_W)\n",
    "random_bias = np.random.normal(size=weights[1].shape) / (GRID_H * GRID_W)\n",
    "\n",
    "layer.set_weights([random_kernel, random_bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
