{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:58:51.862306Z",
     "start_time": "2018-12-20T03:58:49.576205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, LeakyReLU, UpSampling2D, InputLayer, Concatenate, Input, merge, concatenate\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the config\n",
    "\n",
    "The network config is in `yolov3.cfg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:58:51.873335Z",
     "start_time": "2018-12-20T03:58:51.865220Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_config(cfg_path, verbose=False):\n",
    "    file = open(cfg_path, 'r')\n",
    "    lines = file.readlines()\n",
    "        \n",
    "    # get rid of comments and blank lines and white space\n",
    "    lines = [x for x in lines if len(x) > 1]\n",
    "    lines = [x for x in lines if x[0] != '#']\n",
    "    lines = [x.rstrip().lstrip() for x in lines]\n",
    "\n",
    "    block = {}\n",
    "    blocks = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if verbose: print('valuating line: %s' % line)\n",
    "        \n",
    "        if line[0] == '[': # new block start\n",
    "            if len(block) != 0: # if the block inst empty (has data) then reset it\n",
    "                blocks.append(block)\n",
    "                block = {}\n",
    "                \n",
    "            block['type'] = line[1:-1]\n",
    "        else:\n",
    "            key, value = line.split('=')\n",
    "            key, value = key.rstrip(), value.lstrip()\n",
    "            block[key] = value\n",
    "\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:58:51.884721Z",
     "start_time": "2018-12-20T03:58:51.876490Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = parse_config('yolov3.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "(add specific info about the model)\n",
    "\n",
    "### Questions:\n",
    "1. Why is padding important in a convolutional layer\n",
    "2. What is `BatchNormalization` and why is it important\n",
    "3. What does `bilinear` mean?\n",
    "4. What is `B X C X H X W` and how is it difforent than other formats\n",
    "5. What is the difforence between route and shortcut and how are they implemented?\n",
    "    * see the degradation problem, and an [explaination here](https://www.quora.com/What-are-shortcut-connections-how-do-they-work-and-what-is-their-role-in-the-paper-Deep-Residual-Learning-for-Image-Recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T04:09:54.851936Z",
     "start_time": "2018-12-20T04:09:38.234180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolutional, index: 0/107\n",
      "convolutional, index: 1/107\n",
      "convolutional, index: 2/107\n",
      "convolutional, index: 3/107\n",
      "shortcut, index: 4/107\n",
      "convolutional, index: 5/107\n",
      "convolutional, index: 6/107\n",
      "convolutional, index: 7/107\n",
      "shortcut, index: 8/107\n",
      "convolutional, index: 9/107\n",
      "convolutional, index: 10/107\n",
      "shortcut, index: 11/107\n",
      "convolutional, index: 12/107\n",
      "convolutional, index: 13/107\n",
      "convolutional, index: 14/107\n",
      "shortcut, index: 15/107\n",
      "convolutional, index: 16/107\n",
      "convolutional, index: 17/107\n",
      "shortcut, index: 18/107\n",
      "convolutional, index: 19/107\n",
      "convolutional, index: 20/107\n",
      "shortcut, index: 21/107\n",
      "convolutional, index: 22/107\n",
      "convolutional, index: 23/107\n",
      "shortcut, index: 24/107\n",
      "convolutional, index: 25/107\n",
      "convolutional, index: 26/107\n",
      "shortcut, index: 27/107\n",
      "convolutional, index: 28/107\n",
      "convolutional, index: 29/107\n",
      "shortcut, index: 30/107\n",
      "convolutional, index: 31/107\n",
      "convolutional, index: 32/107\n",
      "shortcut, index: 33/107\n",
      "convolutional, index: 34/107\n",
      "convolutional, index: 35/107\n",
      "shortcut, index: 36/107\n",
      "convolutional, index: 37/107\n",
      "convolutional, index: 38/107\n",
      "convolutional, index: 39/107\n",
      "shortcut, index: 40/107\n",
      "convolutional, index: 41/107\n",
      "convolutional, index: 42/107\n",
      "shortcut, index: 43/107\n",
      "convolutional, index: 44/107\n",
      "convolutional, index: 45/107\n",
      "shortcut, index: 46/107\n",
      "convolutional, index: 47/107\n",
      "convolutional, index: 48/107\n",
      "shortcut, index: 49/107\n",
      "convolutional, index: 50/107\n",
      "convolutional, index: 51/107\n",
      "shortcut, index: 52/107\n",
      "convolutional, index: 53/107\n",
      "convolutional, index: 54/107\n",
      "shortcut, index: 55/107\n",
      "convolutional, index: 56/107\n",
      "convolutional, index: 57/107\n",
      "shortcut, index: 58/107\n",
      "convolutional, index: 59/107\n",
      "convolutional, index: 60/107\n",
      "shortcut, index: 61/107\n",
      "convolutional, index: 62/107\n",
      "convolutional, index: 63/107\n",
      "convolutional, index: 64/107\n",
      "shortcut, index: 65/107\n",
      "convolutional, index: 66/107\n",
      "convolutional, index: 67/107\n",
      "shortcut, index: 68/107\n",
      "convolutional, index: 69/107\n",
      "convolutional, index: 70/107\n",
      "shortcut, index: 71/107\n",
      "convolutional, index: 72/107\n",
      "convolutional, index: 73/107\n",
      "shortcut, index: 74/107\n",
      "convolutional, index: 75/107\n",
      "convolutional, index: 76/107\n",
      "convolutional, index: 77/107\n",
      "convolutional, index: 78/107\n",
      "convolutional, index: 79/107\n",
      "convolutional, index: 80/107\n",
      "convolutional, index: 81/107\n",
      "yolo, index: 82/107\n",
      "route, index: 83/107\n",
      "start: -4, end: 0, all_modules: [Tensor(\"leaky_re_lu_638/LeakyRelu:0\", shape=(?, 24, 24, 512), dtype=float32)\n",
      "Tensor(\"leaky_re_lu_639/LeakyRelu:0\", shape=(?, 22, 22, 1024), dtype=float32)\n",
      "Tensor(\"leaky_re_lu_640/LeakyRelu:0\", shape=(?, 22, 22, 512), dtype=float32)\n",
      "Tensor(\"leaky_re_lu_641/LeakyRelu:0\", shape=(?, 20, 20, 1024), dtype=float32)\n",
      "Tensor(\"conv2d_652/BiasAdd:0\", shape=(?, 20, 20, 255), dtype=float32)]\n",
      "convolutional, index: 84/107\n",
      "upsample, index: 85/107\n",
      "route, index: 86/107\n",
      "start: -1, end: 0, all_modules: [Tensor(\"conv2d_652/BiasAdd:0\", shape=(?, 20, 20, 255), dtype=float32)\n",
      "Tensor(\"leaky_re_lu_642/LeakyRelu:0\", shape=(?, 20, 20, 256), dtype=float32)]\n",
      "convolutional, index: 87/107\n",
      "convolutional, index: 88/107\n",
      "convolutional, index: 89/107\n",
      "convolutional, index: 90/107\n",
      "convolutional, index: 91/107\n",
      "convolutional, index: 92/107\n",
      "convolutional, index: 93/107\n",
      "yolo, index: 94/107\n",
      "route, index: 95/107\n",
      "start: -4, end: 0, all_modules: [Tensor(\"leaky_re_lu_645/LeakyRelu:0\", shape=(?, 38, 38, 256), dtype=float32)\n",
      "Tensor(\"leaky_re_lu_646/LeakyRelu:0\", shape=(?, 36, 36, 512), dtype=float32)\n",
      "Tensor(\"leaky_re_lu_647/LeakyRelu:0\", shape=(?, 36, 36, 256), dtype=float32)\n",
      "Tensor(\"leaky_re_lu_648/LeakyRelu:0\", shape=(?, 34, 34, 512), dtype=float32)\n",
      "Tensor(\"conv2d_660/BiasAdd:0\", shape=(?, 34, 34, 255), dtype=float32)]\n",
      "convolutional, index: 96/107\n",
      "upsample, index: 97/107\n",
      "route, index: 98/107\n",
      "start: -1, end: 0, all_modules: [Tensor(\"conv2d_660/BiasAdd:0\", shape=(?, 34, 34, 255), dtype=float32)\n",
      "Tensor(\"leaky_re_lu_649/LeakyRelu:0\", shape=(?, 34, 34, 128), dtype=float32)]\n",
      "convolutional, index: 99/107\n",
      "convolutional, index: 100/107\n",
      "convolutional, index: 101/107\n",
      "convolutional, index: 102/107\n",
      "convolutional, index: 103/107\n",
      "convolutional, index: 104/107\n",
      "convolutional, index: 105/107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'input_11:0' shape=(?, 608, 608, 3) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_584/LeakyRelu:0' shape=(?, 606, 606, 32) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_585/LeakyRelu:0' shape=(?, 302, 302, 64) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_586/LeakyRelu:0' shape=(?, 302, 302, 32) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_587/LeakyRelu:0' shape=(?, 300, 300, 64) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_584/LeakyRelu:0' shape=(?, 606, 606, 32) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_588/LeakyRelu:0' shape=(?, 302, 302, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_589/LeakyRelu:0' shape=(?, 302, 302, 64) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_590/LeakyRelu:0' shape=(?, 300, 300, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_584/LeakyRelu:0' shape=(?, 606, 606, 32) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_591/LeakyRelu:0' shape=(?, 606, 606, 64) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_592/LeakyRelu:0' shape=(?, 604, 604, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_590/LeakyRelu:0' shape=(?, 300, 300, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_593/LeakyRelu:0' shape=(?, 149, 149, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_594/LeakyRelu:0' shape=(?, 149, 149, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_595/LeakyRelu:0' shape=(?, 147, 147, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_590/LeakyRelu:0' shape=(?, 300, 300, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_596/LeakyRelu:0' shape=(?, 300, 300, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_597/LeakyRelu:0' shape=(?, 298, 298, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_595/LeakyRelu:0' shape=(?, 147, 147, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_598/LeakyRelu:0' shape=(?, 147, 147, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_599/LeakyRelu:0' shape=(?, 145, 145, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_597/LeakyRelu:0' shape=(?, 298, 298, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_600/LeakyRelu:0' shape=(?, 298, 298, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_601/LeakyRelu:0' shape=(?, 296, 296, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_599/LeakyRelu:0' shape=(?, 145, 145, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_602/LeakyRelu:0' shape=(?, 145, 145, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_603/LeakyRelu:0' shape=(?, 143, 143, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_601/LeakyRelu:0' shape=(?, 296, 296, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_604/LeakyRelu:0' shape=(?, 296, 296, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_605/LeakyRelu:0' shape=(?, 294, 294, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_603/LeakyRelu:0' shape=(?, 143, 143, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_606/LeakyRelu:0' shape=(?, 143, 143, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_607/LeakyRelu:0' shape=(?, 141, 141, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_605/LeakyRelu:0' shape=(?, 294, 294, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_608/LeakyRelu:0' shape=(?, 294, 294, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_609/LeakyRelu:0' shape=(?, 292, 292, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_607/LeakyRelu:0' shape=(?, 141, 141, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_610/LeakyRelu:0' shape=(?, 70, 70, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_611/LeakyRelu:0' shape=(?, 70, 70, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_612/LeakyRelu:0' shape=(?, 68, 68, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_607/LeakyRelu:0' shape=(?, 141, 141, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_613/LeakyRelu:0' shape=(?, 141, 141, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_614/LeakyRelu:0' shape=(?, 139, 139, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_612/LeakyRelu:0' shape=(?, 68, 68, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_615/LeakyRelu:0' shape=(?, 68, 68, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_616/LeakyRelu:0' shape=(?, 66, 66, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_614/LeakyRelu:0' shape=(?, 139, 139, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_617/LeakyRelu:0' shape=(?, 139, 139, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_618/LeakyRelu:0' shape=(?, 137, 137, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_616/LeakyRelu:0' shape=(?, 66, 66, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_619/LeakyRelu:0' shape=(?, 66, 66, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_620/LeakyRelu:0' shape=(?, 64, 64, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_618/LeakyRelu:0' shape=(?, 137, 137, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_621/LeakyRelu:0' shape=(?, 137, 137, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_622/LeakyRelu:0' shape=(?, 135, 135, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_620/LeakyRelu:0' shape=(?, 64, 64, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_623/LeakyRelu:0' shape=(?, 64, 64, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_624/LeakyRelu:0' shape=(?, 62, 62, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_622/LeakyRelu:0' shape=(?, 135, 135, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_625/LeakyRelu:0' shape=(?, 135, 135, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_626/LeakyRelu:0' shape=(?, 133, 133, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_624/LeakyRelu:0' shape=(?, 62, 62, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_627/LeakyRelu:0' shape=(?, 30, 30, 1024) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_628/LeakyRelu:0' shape=(?, 30, 30, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_629/LeakyRelu:0' shape=(?, 28, 28, 1024) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_624/LeakyRelu:0' shape=(?, 62, 62, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_630/LeakyRelu:0' shape=(?, 62, 62, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_631/LeakyRelu:0' shape=(?, 60, 60, 1024) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_629/LeakyRelu:0' shape=(?, 28, 28, 1024) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_632/LeakyRelu:0' shape=(?, 28, 28, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_633/LeakyRelu:0' shape=(?, 26, 26, 1024) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_631/LeakyRelu:0' shape=(?, 60, 60, 1024) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_634/LeakyRelu:0' shape=(?, 60, 60, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_635/LeakyRelu:0' shape=(?, 58, 58, 1024) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_633/LeakyRelu:0' shape=(?, 26, 26, 1024) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_636/LeakyRelu:0' shape=(?, 26, 26, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_637/LeakyRelu:0' shape=(?, 24, 24, 1024) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_638/LeakyRelu:0' shape=(?, 24, 24, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_639/LeakyRelu:0' shape=(?, 22, 22, 1024) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_640/LeakyRelu:0' shape=(?, 22, 22, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_641/LeakyRelu:0' shape=(?, 20, 20, 1024) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_652/BiasAdd:0' shape=(?, 20, 20, 255) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_652/BiasAdd:0' shape=(?, 20, 20, 255) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_652/BiasAdd:0' shape=(?, 20, 20, 255) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_642/LeakyRelu:0' shape=(?, 20, 20, 256) dtype=float32>,\n",
       " <tf.Tensor 'up_sampling2d_4/ResizeBilinear:0' shape=(?, 40, 40, 256) dtype=float32>,\n",
       " <tf.Tensor 'up_sampling2d_4/ResizeBilinear:0' shape=(?, 40, 40, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_643/LeakyRelu:0' shape=(?, 40, 40, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_644/LeakyRelu:0' shape=(?, 38, 38, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_645/LeakyRelu:0' shape=(?, 38, 38, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_646/LeakyRelu:0' shape=(?, 36, 36, 512) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_647/LeakyRelu:0' shape=(?, 36, 36, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_648/LeakyRelu:0' shape=(?, 34, 34, 512) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_660/BiasAdd:0' shape=(?, 34, 34, 255) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_660/BiasAdd:0' shape=(?, 34, 34, 255) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_660/BiasAdd:0' shape=(?, 34, 34, 255) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_649/LeakyRelu:0' shape=(?, 34, 34, 128) dtype=float32>,\n",
       " <tf.Tensor 'up_sampling2d_5/ResizeBilinear:0' shape=(?, 68, 68, 128) dtype=float32>,\n",
       " <tf.Tensor 'up_sampling2d_5/ResizeBilinear:0' shape=(?, 68, 68, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_650/LeakyRelu:0' shape=(?, 68, 68, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_651/LeakyRelu:0' shape=(?, 66, 66, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_652/LeakyRelu:0' shape=(?, 66, 66, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_653/LeakyRelu:0' shape=(?, 64, 64, 256) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_654/LeakyRelu:0' shape=(?, 64, 64, 128) dtype=float32>,\n",
       " <tf.Tensor 'leaky_re_lu_655/LeakyRelu:0' shape=(?, 62, 62, 256) dtype=float32>,\n",
       " <tf.Tensor 'conv2d_668/BiasAdd:0' shape=(?, 62, 62, 255) dtype=float32>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_modle = Sequential()\n",
    "\n",
    "all_modules = [Input((608, 608, 3))] # add InputLayer for sequential\n",
    "module = all_modules[-1]\n",
    "\n",
    "for index, block in enumerate(blocks[1:]): # block 0 specifies network info\n",
    "    print('%s, index: %i/%i' % (block['type'], index, len(blocks)))\n",
    "    \n",
    "    ### Conv(ish) layers\n",
    "    if block['type'] == 'convolutional':\n",
    "        # layer config\n",
    "        activation = block['activation']\n",
    "        \n",
    "        bais = False\n",
    "        try:\n",
    "            batch_normalize = int(block['batch_normalize'])\n",
    "        except:\n",
    "            batch_normalize = 0\n",
    "            bais = True\n",
    "            \n",
    "        filters = int(block['filters'])\n",
    "        padding = int(block['pad'])\n",
    "        kerel_size = int(block['size'])\n",
    "        strides = int(block['stride'])\n",
    "        \n",
    "# not working in keras\n",
    "#         if padding:\n",
    "#             pad = (kernel_size - 1) // 2\n",
    "#         else:\n",
    "#             pad = 0\n",
    "        \n",
    "        # create the conv layer\n",
    "        module = Conv2D(filters, kerel_size, strides=strides, use_bias=bais)(module)\n",
    "        \n",
    "        if batch_normalize:\n",
    "            module = BatchNormalization()(module)\n",
    "        \n",
    "        if activation == 'leaky':\n",
    "            module = LeakyReLU(0.1)(module)\n",
    "            \n",
    "    elif block['type'] == 'upsample':\n",
    "        stride = int(block['stride'])\n",
    "        module = UpSampling2D(size=(stride, stride), interpolation='bilinear')(module)\n",
    "\n",
    "    ### Routs and shortcuts    \n",
    "    elif block['type'] == 'route': # we need to connect these in the right way\n",
    "        layers = block['layers'].split(',')\n",
    "        start = int(layers[0])\n",
    "        \n",
    "        try:\n",
    "           end = int(blcok['layers'][1]) \n",
    "        except: # if there is no end\n",
    "            end = 0\n",
    "        \n",
    "        if start > 0:\n",
    "            start = start - index\n",
    "        if end > 0:\n",
    "            end = end - index\n",
    "        \n",
    "#         from IPython.core.debugger import Tracer; Tracer()()\n",
    "        \n",
    "        print('start: %d, end: %d, all_modules: [%s]' % (start, end, '\\n'.join(map(str, all_modules[index + start - 1: index + end]))))\n",
    "    \n",
    "        l1 = all_modules[index + start]\n",
    "        l2 = all_modules[index + end]\n",
    "        \n",
    "        if l1.shape[1] == l2.shape[1]: # hack to make sure tensors fit\n",
    "            module = concatenate([l1, l2])\n",
    "    \n",
    "    elif block['type'] == 'shortcut':\n",
    "        f = int(block['from'])\n",
    "#         from IPython.core.debugger import Tracer; Tracer()()\n",
    "        module = all_modules[index + f]\n",
    "\n",
    "    elif block['type'] == 'yolo': # this needs to also be connected to the network\n",
    "        mask = block['mask'].split(',')\n",
    "        mask = [int(x) for x in mask] # convert to ints\n",
    "        \n",
    "        anchors = block['anchors'].split(',')\n",
    "        anchors = [int(a) for a in anchors]\n",
    "        anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)] # break anchors into pairs\n",
    "        anchors = [anchors[i] for i in mask]\n",
    "    \n",
    "    all_modules += [module]\n",
    "\n",
    "all_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T04:13:17.243748Z",
     "start_time": "2018-12-20T04:13:17.209170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 608, 608, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_594 (Conv2D)          (None, 606, 606, 32)      864       \n",
      "_________________________________________________________________\n",
      "batch_normalization_584 (Bat (None, 606, 606, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_584 (LeakyReLU)  (None, 606, 606, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_598 (Conv2D)          (None, 302, 302, 128)     36864     \n",
      "_________________________________________________________________\n",
      "batch_normalization_588 (Bat (None, 302, 302, 128)     512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_588 (LeakyReLU)  (None, 302, 302, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_599 (Conv2D)          (None, 302, 302, 64)      8192      \n",
      "_________________________________________________________________\n",
      "batch_normalization_589 (Bat (None, 302, 302, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_589 (LeakyReLU)  (None, 302, 302, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_600 (Conv2D)          (None, 300, 300, 128)     73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_590 (Bat (None, 300, 300, 128)     512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_590 (LeakyReLU)  (None, 300, 300, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_603 (Conv2D)          (None, 149, 149, 256)     294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_593 (Bat (None, 149, 149, 256)     1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_593 (LeakyReLU)  (None, 149, 149, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_604 (Conv2D)          (None, 149, 149, 128)     32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_594 (Bat (None, 149, 149, 128)     512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_594 (LeakyReLU)  (None, 149, 149, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_605 (Conv2D)          (None, 147, 147, 256)     294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_595 (Bat (None, 147, 147, 256)     1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_595 (LeakyReLU)  (None, 147, 147, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_608 (Conv2D)          (None, 147, 147, 128)     32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_598 (Bat (None, 147, 147, 128)     512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_598 (LeakyReLU)  (None, 147, 147, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_609 (Conv2D)          (None, 145, 145, 256)     294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_599 (Bat (None, 145, 145, 256)     1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_599 (LeakyReLU)  (None, 145, 145, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_612 (Conv2D)          (None, 145, 145, 128)     32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_602 (Bat (None, 145, 145, 128)     512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_602 (LeakyReLU)  (None, 145, 145, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_613 (Conv2D)          (None, 143, 143, 256)     294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_603 (Bat (None, 143, 143, 256)     1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_603 (LeakyReLU)  (None, 143, 143, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_616 (Conv2D)          (None, 143, 143, 128)     32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_606 (Bat (None, 143, 143, 128)     512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_606 (LeakyReLU)  (None, 143, 143, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_617 (Conv2D)          (None, 141, 141, 256)     294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_607 (Bat (None, 141, 141, 256)     1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_607 (LeakyReLU)  (None, 141, 141, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_620 (Conv2D)          (None, 70, 70, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_610 (Bat (None, 70, 70, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_610 (LeakyReLU)  (None, 70, 70, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_621 (Conv2D)          (None, 70, 70, 256)       131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_611 (Bat (None, 70, 70, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_611 (LeakyReLU)  (None, 70, 70, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_622 (Conv2D)          (None, 68, 68, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_612 (Bat (None, 68, 68, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_612 (LeakyReLU)  (None, 68, 68, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_625 (Conv2D)          (None, 68, 68, 256)       131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_615 (Bat (None, 68, 68, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_615 (LeakyReLU)  (None, 68, 68, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_626 (Conv2D)          (None, 66, 66, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_616 (Bat (None, 66, 66, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_616 (LeakyReLU)  (None, 66, 66, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_629 (Conv2D)          (None, 66, 66, 256)       131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_619 (Bat (None, 66, 66, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_619 (LeakyReLU)  (None, 66, 66, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_630 (Conv2D)          (None, 64, 64, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_620 (Bat (None, 64, 64, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_620 (LeakyReLU)  (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_633 (Conv2D)          (None, 64, 64, 256)       131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_623 (Bat (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_623 (LeakyReLU)  (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_634 (Conv2D)          (None, 62, 62, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_624 (Bat (None, 62, 62, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_624 (LeakyReLU)  (None, 62, 62, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_637 (Conv2D)          (None, 30, 30, 1024)      4718592   \n",
      "_________________________________________________________________\n",
      "batch_normalization_627 (Bat (None, 30, 30, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_627 (LeakyReLU)  (None, 30, 30, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_638 (Conv2D)          (None, 30, 30, 512)       524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_628 (Bat (None, 30, 30, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_628 (LeakyReLU)  (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_639 (Conv2D)          (None, 28, 28, 1024)      4718592   \n",
      "_________________________________________________________________\n",
      "batch_normalization_629 (Bat (None, 28, 28, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_629 (LeakyReLU)  (None, 28, 28, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_642 (Conv2D)          (None, 28, 28, 512)       524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_632 (Bat (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_632 (LeakyReLU)  (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_643 (Conv2D)          (None, 26, 26, 1024)      4718592   \n",
      "_________________________________________________________________\n",
      "batch_normalization_633 (Bat (None, 26, 26, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_633 (LeakyReLU)  (None, 26, 26, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_646 (Conv2D)          (None, 26, 26, 512)       524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_636 (Bat (None, 26, 26, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_636 (LeakyReLU)  (None, 26, 26, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_647 (Conv2D)          (None, 24, 24, 1024)      4718592   \n",
      "_________________________________________________________________\n",
      "batch_normalization_637 (Bat (None, 24, 24, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_637 (LeakyReLU)  (None, 24, 24, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_648 (Conv2D)          (None, 24, 24, 512)       524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_638 (Bat (None, 24, 24, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_638 (LeakyReLU)  (None, 24, 24, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_649 (Conv2D)          (None, 22, 22, 1024)      4718592   \n",
      "_________________________________________________________________\n",
      "batch_normalization_639 (Bat (None, 22, 22, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_639 (LeakyReLU)  (None, 22, 22, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_650 (Conv2D)          (None, 22, 22, 512)       524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_640 (Bat (None, 22, 22, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_640 (LeakyReLU)  (None, 22, 22, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_651 (Conv2D)          (None, 20, 20, 1024)      4718592   \n",
      "_________________________________________________________________\n",
      "batch_normalization_641 (Bat (None, 20, 20, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_641 (LeakyReLU)  (None, 20, 20, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_652 (Conv2D)          (None, 20, 20, 255)       261375    \n",
      "_________________________________________________________________\n",
      "conv2d_653 (Conv2D)          (None, 20, 20, 256)       65280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_642 (Bat (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_642 (LeakyReLU)  (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 40, 40, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_654 (Conv2D)          (None, 40, 40, 256)       65536     \n",
      "_________________________________________________________________\n",
      "batch_normalization_643 (Bat (None, 40, 40, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_643 (LeakyReLU)  (None, 40, 40, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_655 (Conv2D)          (None, 38, 38, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_644 (Bat (None, 38, 38, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_644 (LeakyReLU)  (None, 38, 38, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_656 (Conv2D)          (None, 38, 38, 256)       131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_645 (Bat (None, 38, 38, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_645 (LeakyReLU)  (None, 38, 38, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_657 (Conv2D)          (None, 36, 36, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_646 (Bat (None, 36, 36, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_646 (LeakyReLU)  (None, 36, 36, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_658 (Conv2D)          (None, 36, 36, 256)       131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_647 (Bat (None, 36, 36, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_647 (LeakyReLU)  (None, 36, 36, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_659 (Conv2D)          (None, 34, 34, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_648 (Bat (None, 34, 34, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_648 (LeakyReLU)  (None, 34, 34, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_660 (Conv2D)          (None, 34, 34, 255)       130815    \n",
      "_________________________________________________________________\n",
      "conv2d_661 (Conv2D)          (None, 34, 34, 128)       32640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_649 (Bat (None, 34, 34, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_649 (LeakyReLU)  (None, 34, 34, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 68, 68, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_662 (Conv2D)          (None, 68, 68, 128)       16384     \n",
      "_________________________________________________________________\n",
      "batch_normalization_650 (Bat (None, 68, 68, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_650 (LeakyReLU)  (None, 68, 68, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_663 (Conv2D)          (None, 66, 66, 256)       294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_651 (Bat (None, 66, 66, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_651 (LeakyReLU)  (None, 66, 66, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_664 (Conv2D)          (None, 66, 66, 128)       32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_652 (Bat (None, 66, 66, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_652 (LeakyReLU)  (None, 66, 66, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_665 (Conv2D)          (None, 64, 64, 256)       294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_653 (Bat (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_653 (LeakyReLU)  (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_666 (Conv2D)          (None, 64, 64, 128)       32768     \n",
      "_________________________________________________________________\n",
      "batch_normalization_654 (Bat (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_654 (LeakyReLU)  (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_667 (Conv2D)          (None, 62, 62, 256)       294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_655 (Bat (None, 62, 62, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_655 (LeakyReLU)  (None, 62, 62, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_668 (Conv2D)          (None, 62, 62, 255)       65535     \n",
      "=================================================================\n",
      "Total params: 44,542,813\n",
      "Trainable params: 44,506,269\n",
      "Non-trainable params: 36,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_modle = Model(all_modules[0], module)\n",
    "main_modle.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
